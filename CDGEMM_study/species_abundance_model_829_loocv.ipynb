{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"YH20TaYMfOPx"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","    os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5mHH-frLYN_I"},"outputs":[],"source":["# Load packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import re\n","from collections import Counter, defaultdict\n","from joblib import Parallel, delayed\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectKBest, f_classif, RFE\n","from sklearn.linear_model import LassoCV, LogisticRegression, LogisticRegressionCV, ElasticNetCV, ElasticNet\n","from sklearn.metrics import f1_score, roc_auc_score\n","from sklearn.model_selection import LeaveOneOut, KFold, GridSearchCV, StratifiedKFold\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","import statistics\n","import xgboost as xgb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":5126,"status":"ok","timestamp":1716746521218,"user":{"displayName":"ZHIQI ZHANG","userId":"11425140851942185431"},"user_tz":240},"id":"zO7a90AvYqwL","outputId":"03b794b3-887f-4421-eda9-22048738a493"},"outputs":[],"source":["# Load dataset\n","file_path = 'species_abundance_merged_2024-08-13.xlsx'\n","new_data = pd.read_excel(file_path)\n","new_data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Exclude subjects with a CD onset of 12 months\n","excluded_subjects = [23, 31]\n","new_data = new_data[~new_data['Subject_number'].isin(excluded_subjects)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to apply abundance and prevalence thresholds\n","def filter_species(data, abundance_threshold = 0.001, prevalence_threshold = 0.1):\n","    initial_species_count = data.shape[1] - 8\n","    sample_count = data.shape[0]\n","    \n","    # Calculate prevalence threshold\n","    min_prevalent_samples = int(prevalence_threshold * sample_count)\n","\n","    # Filter species based on the thresholds\n","    species_columns = data.columns.difference(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'])\n","    species_data = data[species_columns]\n","    \n","    # Calculate the abundance and prevalence for each species\n","    species_above_threshold = (species_data >= abundance_threshold).sum(axis=0) >= min_prevalent_samples\n","    filtered_species = species_columns[species_above_threshold]\n","    \n","    # Filter the data to keep only the selected species\n","    filtered_data = data[['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'] + filtered_species.tolist()]\n","\n","    final_species_count = len(filtered_species)\n","    print(f\"Initial number of species: {initial_species_count}\")\n","    print(f\"Number of species after filtering: {final_species_count}\")\n","    \n","    return filtered_data\n","\n","# Filter the data\n","new_data_filtered = filter_species(new_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6178050,"status":"ok","timestamp":1716315664250,"user":{"displayName":"ZHIQI ZHANG","userId":"11425140851942185431"},"user_tz":240},"id":"F5_nys1ArUKp","outputId":"3952a28d-4c32-4b32-d3b7-b22f3ea861f1"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","# Define a function to evaluate combinations of feature selectors and models\n","def evaluate_models(data, max_timepoint):\n","    results = {}\n","\n","    # Define a function to process each time point independently\n","    def process_time_point(time_point):\n","        print(f\"Processing time point: {time_point}\")\n","\n","        # Filter data for the current time point and extract selected features\n","        current_data = data[data['timepoint_numeric'] == time_point]\n","        X = current_data.drop(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'], axis=1) # Feature matrix\n","        y = current_data['Diagnosis'] # Labels\n","\n","        # Ensure there are enough samples to perform LOOCV\n","        if len(y) < 2:\n","            print(f\"Skipping time point {time_point} due to insufficient samples.\")\n","            return time_point, None\n","\n","        # Initial feature reduction to 100 features\n","        selector = SelectKBest(score_func=f_classif, k=100)\n","        X_reduced = selector.fit_transform(X, y)\n","        selected_feature_names = X.columns[selector.get_support(indices=True)]\n","        X_reduced = pd.DataFrame(X_reduced, columns=selected_feature_names)\n","\n","        # Initialize Leave-One-Out cross-validation\n","        loo = LeaveOneOut()\n","        best_overall_score = 0\n","        best_overall_setup = {}\n","\n","        # Define a function to process each feature selector and model combination\n","        def process_combination(feature_selector_name, ml_model_name):\n","            all_selected_features = []\n","            all_importances = []\n","\n","            # Loop through the training/test splits generated by LOOCV\n","            for train_index, test_index in loo.split(X_reduced):\n","                X_train, X_test = X_reduced.iloc[train_index], X_reduced.iloc[test_index]\n","                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                # Perform feature selection based on the specified selector\n","                if feature_selector_name == 'LASSO':\n","                    feature_selector = LassoCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30)).fit(X_train, y_train)\n","                else:\n","                    feature_selector = ElasticNetCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7).fit(X_train, y_train)\n","\n","                # Select features with non-zero coefficients\n","                selected_features = X_train.columns[feature_selector.coef_ != 0]\n","                selected_features = selected_features[:min(len(selected_features), int(0.8 * len(y_train)))]\n","\n","                # If no features are selected, skip this iteration\n","                if len(selected_features) == 0:\n","                    continue\n","\n","                # Select the relevant features from the training set\n","                X_train_selected = X_train[selected_features]\n","\n","                # Perform logistic regression for ranking the selected features based on importance\n","                logistic = LogisticRegression(max_iter=10000, random_state=42, solver='liblinear').fit(X_train_selected, y_train)\n","                importances = abs(logistic.coef_[0])\n","                ranked_features = sorted(zip(selected_features, importances), key=lambda x: x[1], reverse=True)\n","\n","                # Store selected features and their importance\n","                all_selected_features.extend([f[0] for f in ranked_features])\n","                all_importances.extend([f[1] for f in ranked_features])\n","\n","            # If no features were selected across all folds, return None\n","            if not all_selected_features:\n","                return None\n","\n","            # Aggregate selected features across all folds\n","            unique_features = list(set(all_selected_features))\n","            frequency = Counter(all_selected_features)\n","            avg_importance = {feature: np.mean([imp for feat, imp in zip(all_selected_features, all_importances) if feat == feature])\n","                              for feature in unique_features}\n","\n","            # Calculate a composite score for each feature based on frequency and importance\n","            composite_scores = {feature: 0.5 * (frequency[feature] / loo.get_n_splits(X_reduced)) + 0.5 * (avg_importance[feature] / sum(avg_importance.values()))\n","                                for feature in unique_features}\n","            sorted_features = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","            # Evaluate different feature subsets with varying thresholds\n","            best_overall_performance = 0\n","            best_overall_setup = {}\n","            best_percentage = 0\n","            thresholds = np.linspace(0.05, 0.95, 19)\n","\n","            for i in range(1, min(len(sorted_features), int(0.8 * len(y))) + 1):\n","                selected_features = [feature[0] for feature in sorted_features[:i]]\n","                best_performance_for_features = 0\n","                best_threshold_for_features = None\n","\n","                for threshold in thresholds:\n","                    fold_f1_scores = []\n","\n","                    # Perform LOOCV prediction with the selected features and model\n","                    for train_index, test_index in loo.split(X_reduced):\n","                        X_train, X_test = X_reduced.iloc[train_index][selected_features], X_reduced.iloc[test_index][selected_features]\n","                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                        # Use the appropriate model for prediction\n","                        if ml_model_name == 'RandomForest':\n","                            model = RandomForestClassifier(n_estimators=300, random_state=42)\n","                        else:\n","                            model = ElasticNetCV(cv=5, max_iter=10000, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7)\n","\n","                        # Fit the model, make predictions and compute F1 score for the current fold\n","                        model.fit(X_train, y_train)\n","                        test_prediction = (model.predict_proba(X_test)[:, 1] >= threshold).astype(int) if ml_model_name == 'RandomForest' else (model.predict(X_test) >= threshold).astype(int)\n","                        f1_score_current = f1_score(y_test, test_prediction, average='macro')\n","                        fold_f1_scores.append(f1_score_current)\n","\n","                    # Calculate the average F1 score for the current threshold\n","                    f1_score_avg = np.mean(fold_f1_scores)\n","\n","                    # Record the best performance and threshold for the features\n","                    if f1_score_avg > best_performance_for_features:\n","                        best_performance_for_features = f1_score_avg\n","                        best_threshold_for_features = threshold\n","\n","                # Update the best overall performance if it improves\n","                if best_performance_for_features > best_overall_performance:\n","                    best_overall_performance = best_performance_for_features\n","                    best_overall_setup = {\n","                        'features': selected_features,\n","                        'threshold': best_threshold_for_features,\n","                        'performance': best_performance_for_features,\n","                        'feature_selection_method': feature_selector_name,\n","                        'ml_model': ml_model_name\n","                    }\n","\n","            return {\n","                'feature_selection_method': best_overall_setup['feature_selection_method'],\n","                'ml_model': best_overall_setup['ml_model'],\n","                'best_features': best_overall_setup['features'],\n","                'features_length': len(best_overall_setup['features']),\n","                'best_threshold': best_overall_setup['threshold'],\n","                'best_performance': best_overall_setup['performance']\n","            }\n","\n","        # Process combinations of feature selection and prediction models\n","        combinations = [('LASSO', 'ElasticNet'), ('ElasticNet', 'ElasticNet')]\n","        results_per_combination = [process_combination(fs, ml) for fs, ml in combinations]\n","        best_combination = max(results_per_combination, key=lambda x: x['best_performance'] if x is not None else 0)\n","\n","        return time_point, best_combination\n","\n","    # Process time points within the specified max_timepoint\n","    time_points = np.sort(data[data['timepoint_numeric'] < max_timepoint]['timepoint_numeric'].unique())\n","    results_parallel = Parallel(n_jobs=-1)(delayed(process_time_point)(tp) for tp in time_points)\n","    results = {tp: result for tp, result in results_parallel if result is not None}\n","\n","    return results\n","\n","# Set the max timepoint to consider data before the earliest onset age\n","max_timepoint = 18\n","\n","# Evaluate models for all subjects\n","print(\"Evaluating Celiac Disease Prediction\")\n","results_ced = evaluate_models(new_data_filtered, max_timepoint)\n","\n","# Print results for all subjects\n","print(\"Results for Celiac Disease Prediction for All Subjects:\")\n","for time_point, res in results_ced.items():\n","    if res is not None:\n","        print(f\"Time Point: {time_point}\")\n","        print(f\"  Feature Selection Method: {res['feature_selection_method']}\")\n","        print(f\"  Machine Learning Model: {res['ml_model']}\")\n","        print(f\"  Best F1 Score: {res['best_performance']}\")\n","        print(f\"  Best Threshold: {res['best_threshold']}\")\n","        print(f\"  Features Used: {res['best_features']}\")\n","        print(f\"  Features Length: {res['features_length']}\")\n","        print(\"-\" * 40)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","# Filter data for celiac disease cases before 18 and 36 months\n","celiac_data = new_data_filtered[(new_data_filtered['Diagnosis'] == 1) & \n","                                (new_data_filtered['timepoint_numeric'] <= 15)]\n","\n","# Define early onset vs. late onset based on the thresholds (18 months and 36 months)\n","celiac_data['Onset_Type'] = np.where(celiac_data['CD_onset'] <= 18, 'Early', 'Late')\n","celiac_data['Onset_Type'] = np.where(celiac_data['CD_onset'] <= 36, celiac_data['Onset_Type'], 'None')\n","\n","# Remove rows where Onset_Type is 'None' (for onset after 36 months)\n","celiac_data = celiac_data[celiac_data['Onset_Type'] != 'None']\n","\n","# Encode 'Onset_Type' to numerical values: Early = 0, Late = 1\n","celiac_data['Onset_Type'] = celiac_data['Onset_Type'].map({'Early': 0, 'Late': 1})\n","\n","# Define a function to evaluate combinations of feature selectors and models\n","def evaluate_models(data):\n","    results = {}\n","\n","    # Define a function to process each time point independently\n","    def process_time_point(time_point):\n","        print(f\"Processing time point: {time_point}\")\n","\n","        # Filter data for the current time point and extract selected features\n","        current_data = data[data['timepoint_numeric'] == time_point]\n","        X = current_data.drop(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country', 'Onset_Type'], axis=1) # Feature matrix\n","        y = current_data['Onset_Type'] # Labels\n","\n","        # Ensure there are enough samples to perform LOOCV\n","        if len(y) < 2:\n","            print(f\"Skipping time point {time_point} due to insufficient samples.\")\n","            return time_point, None\n","\n","        # Initial feature reduction to 100 features\n","        selector = SelectKBest(score_func=f_classif, k=100)\n","        X_reduced = selector.fit_transform(X, y)\n","        selected_feature_names = X.columns[selector.get_support(indices=True)]\n","        X_reduced = pd.DataFrame(X_reduced, columns=selected_feature_names)\n","\n","        # Initialize Leave-One-Out cross-validation\n","        loo = LeaveOneOut()\n","        best_overall_score = 0\n","        best_overall_setup = {}\n","\n","        # Define a function to process each feature selector and model combination\n","        def process_combination(feature_selector_name, ml_model_name):\n","            all_selected_features = []\n","            all_importances = []\n","\n","            # Loop through the training/test splits generated by LOOCV\n","            for train_index, test_index in loo.split(X_reduced):\n","                X_train, X_test = X_reduced.iloc[train_index], X_reduced.iloc[test_index]\n","                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                # Perform feature selection based on the specified selector\n","                if feature_selector_name == 'LASSO':\n","                    feature_selector = LassoCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30)).fit(X_train, y_train)\n","                else:\n","                    feature_selector = ElasticNetCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7).fit(X_train, y_train)\n","\n","                # Select features with non-zero coefficients\n","                selected_features = X_train.columns[feature_selector.coef_ != 0]\n","                selected_features = selected_features[:min(len(selected_features), int(0.8 * len(y_train)))]\n","\n","                # If no features are selected, skip this iteration\n","                if len(selected_features) == 0:\n","                    continue\n","\n","                # Select the relevant features from the training set\n","                X_train_selected = X_train[selected_features]\n","\n","                # Perform logistic regression for ranking the selected features based on importance\n","                logistic = LogisticRegression(max_iter=10000, random_state=42, solver='liblinear').fit(X_train_selected, y_train)\n","                importances = abs(logistic.coef_[0])\n","                ranked_features = sorted(zip(selected_features, importances), key=lambda x: x[1], reverse=True)\n","\n","                # Store selected features and their importance\n","                all_selected_features.extend([f[0] for f in ranked_features])\n","                all_importances.extend([f[1] for f in ranked_features])\n","\n","            # If no features were selected across all folds, return None\n","            if not all_selected_features:\n","                return None\n","\n","            # Aggregate selected features across all folds\n","            unique_features = list(set(all_selected_features))\n","            frequency = Counter(all_selected_features)\n","            avg_importance = {feature: np.mean([imp for feat, imp in zip(all_selected_features, all_importances) if feat == feature])\n","                              for feature in unique_features}\n","\n","            # Calculate a composite score for each feature based on frequency and importance\n","            composite_scores = {feature: 0.5 * (frequency[feature] / loo.get_n_splits(X_reduced)) + 0.5 * (avg_importance[feature] / sum(avg_importance.values()))\n","                                for feature in unique_features}\n","            sorted_features = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","            # Evaluate different feature subsets with varying thresholds\n","            best_overall_performance = 0\n","            best_overall_setup = {}\n","            best_percentage = 0\n","            thresholds = np.linspace(0.05, 0.95, 19)\n","\n","            for i in range(1, min(len(sorted_features), int(0.8 * len(y))) + 1):\n","                selected_features = [feature[0] for feature in sorted_features[:i]]\n","                best_performance_for_features = 0\n","                best_threshold_for_features = None\n","\n","                for threshold in thresholds:\n","                    fold_f1_scores = []\n","\n","                    # Perform LOOCV prediction with the selected features and model\n","                    for train_index, test_index in loo.split(X_reduced):\n","                        X_train, X_test = X_reduced.iloc[train_index][selected_features], X_reduced.iloc[test_index][selected_features]\n","                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                        # Use the appropriate model for prediction\n","                        if ml_model_name == 'RandomForest':\n","                            model = RandomForestClassifier(n_estimators=300, random_state=42)\n","                        else:\n","                            model = ElasticNetCV(cv=5, max_iter=10000, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7)\n","\n","                        # Fit the model, make predictions and compute F1 score for the current fold\n","                        model.fit(X_train, y_train)\n","                        test_prediction = (model.predict_proba(X_test)[:, 1] >= threshold).astype(int) if ml_model_name == 'RandomForest' else (model.predict(X_test) >= threshold).astype(int)\n","                        f1_score_current = f1_score(y_test, test_prediction, average='macro')\n","                        fold_f1_scores.append(f1_score_current)\n","\n","                    # Calculate the average F1 score for the current threshold\n","                    f1_score_avg = np.mean(fold_f1_scores)\n","\n","                    # Record the best performance and threshold for the features\n","                    if f1_score_avg > best_performance_for_features:\n","                        best_performance_for_features = f1_score_avg\n","                        best_threshold_for_features = threshold\n","\n","                # Update the best overall performance if it improves\n","                if best_performance_for_features > best_overall_performance:\n","                    best_overall_performance = best_performance_for_features\n","                    best_overall_setup = {\n","                        'features': selected_features,\n","                        'threshold': best_threshold_for_features,\n","                        'performance': best_performance_for_features,\n","                        'feature_selection_method': feature_selector_name,\n","                        'ml_model': ml_model_name\n","                    }\n","\n","            return {\n","                'feature_selection_method': best_overall_setup['feature_selection_method'],\n","                'ml_model': best_overall_setup['ml_model'],\n","                'best_features': best_overall_setup['features'],\n","                'features_length': len(best_overall_setup['features']),\n","                'best_threshold': best_overall_setup['threshold'],\n","                'best_performance': best_overall_setup['performance']\n","            }\n","\n","        # Process combinations of feature selection and prediction models\n","        combinations = [('LASSO', 'ElasticNet'), ('ElasticNet', 'ElasticNet')]\n","        results_per_combination = [process_combination(fs, ml) for fs, ml in combinations]\n","        best_combination = max(results_per_combination, key=lambda x: x['best_performance'] if x is not None else 0)\n","\n","        return time_point, best_combination\n","\n","    # Process time points within the specified range (before 15 months)\n","    time_points = np.sort(data['timepoint_numeric'].unique())\n","    results_parallel = Parallel(n_jobs=-1)(delayed(process_time_point)(tp) for tp in time_points)\n","    results = {tp: result for tp, result in results_parallel if result is not None}\n","\n","    return results\n","\n","# Evaluate the model\n","results_early_late = evaluate_models(celiac_data)\n","\n","# Print results\n","print(\"Results for Early Onset vs. Late Onset Prediction:\")\n","for time_point, res in results_early_late.items():\n","    if res is not None:\n","        print(f\"Time Point: {time_point}\")\n","        print(f\"  Feature Selection Method: {res['feature_selection_method']}\")\n","        print(f\"  Machine Learning Model: {res['ml_model']}\")\n","        print(f\"  Best F1 Score: {res['best_performance']}\")\n","        print(f\"  Best Threshold: {res['best_threshold']}\")\n","        print(f\"  Features Used: {res['best_features']}\")\n","        print(f\"  Features Length: {res['features_length']}\")\n","        print(\"-\" * 40)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","# Filter data to include only relative time points from -18 to onset\n","filtered_data = new_data_filtered[(new_data_filtered['Relative_timepoint'] >= -18) & (new_data_filtered['Relative_timepoint'] <= 0)]\n","\n","# Define a function to evaluate combinations of feature selectors and models\n","def evaluate_models(data):\n","    results = {}\n","\n","    # Define a function to process each time point independently\n","    def process_time_point(time_point):\n","        print(f\"Processing relative time point: {time_point}\")\n","\n","        # Filter data for the current relative time point and extract selected features\n","        current_data = data[data['Relative_timepoint'] == time_point]\n","        X = current_data.drop(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'], axis=1) # Feature matrix\n","        y = current_data['Diagnosis'] # Labels\n","\n","        # Ensure there are enough samples to perform LOOCV\n","        if len(y) < 2:\n","            print(f\"Skipping relative time point {time_point} due to insufficient samples.\")\n","            return time_point, None\n","\n","        # Initial feature reduction to 100 features\n","        selector = SelectKBest(score_func=f_classif, k=100)\n","        X_reduced = selector.fit_transform(X, y)\n","        selected_feature_names = X.columns[selector.get_support(indices=True)]\n","        X_reduced = pd.DataFrame(X_reduced, columns=selected_feature_names)\n","\n","        # Initialize Leave-One-Out cross-validation\n","        loo = LeaveOneOut()\n","        best_overall_score = 0\n","        best_overall_setup = {}\n","\n","        # Define a function to process each feature selector and model combination\n","        def process_combination(feature_selector_name, ml_model_name):\n","            all_selected_features = []\n","            all_importances = []\n","\n","            # Loop through the training/test splits generated by LOOCV\n","            for train_index, test_index in loo.split(X_reduced):\n","                X_train, X_test = X_reduced.iloc[train_index], X_reduced.iloc[test_index]\n","                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                # Perform feature selection based on the specified selector\n","                if feature_selector_name == 'LASSO':\n","                    feature_selector = LassoCV(cv=loo, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30)).fit(X_train, y_train)\n","                else:\n","                    feature_selector = ElasticNetCV(cv=loo, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7).fit(X_train, y_train)\n","\n","                # Select features with non-zero coefficients\n","                selected_features = X_train.columns[feature_selector.coef_ != 0]\n","                selected_features = selected_features[:min(len(selected_features), int(0.8 * len(y_train)))]\n","\n","                # If no features are selected, skip this iteration\n","                if len(selected_features) == 0:\n","                    continue\n","\n","                # Select the relevant features from the training set\n","                X_train_selected = X_train[selected_features]\n","\n","                # Perform logistic regression for ranking the selected features based on importance\n","                logistic = LogisticRegression(max_iter=10000, random_state=42, solver='liblinear').fit(X_train_selected, y_train)\n","                importances = abs(logistic.coef_[0])\n","                ranked_features = sorted(zip(selected_features, importances), key=lambda x: x[1], reverse=True)\n","\n","                # Store selected features and their importance\n","                all_selected_features.extend([f[0] for f in ranked_features])\n","                all_importances.extend([f[1] for f in ranked_features])\n","\n","            # If no features were selected across all folds, return None\n","            if not all_selected_features:\n","                return None\n","\n","            # Aggregate selected features across all folds\n","            unique_features = list(set(all_selected_features))\n","            frequency = Counter(all_selected_features)\n","            avg_importance = {feature: np.mean([imp for feat, imp in zip(all_selected_features, all_importances) if feat == feature])\n","                              for feature in unique_features}\n","\n","            # Calculate a composite score for each feature based on frequency and importance\n","            composite_scores = {feature: 0.5 * (frequency[feature] / loo.get_n_splits(X_reduced)) + 0.5 * (avg_importance[feature] / sum(avg_importance.values()))\n","                                for feature in unique_features}\n","            sorted_features = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","            # Evaluate different feature subsets with varying thresholds\n","            best_overall_performance = 0\n","            best_overall_setup = {}\n","            best_percentage = 0\n","            thresholds = np.linspace(0.05, 0.95, 19)\n","\n","            for i in range(1, min(len(sorted_features), int(0.8 * len(y))) + 1):\n","                selected_features = [feature[0] for feature in sorted_features[:i]]\n","                best_performance_for_features = 0\n","                best_threshold_for_features = None\n","\n","                for threshold in thresholds:\n","                    fold_f1_scores = []\n","\n","                    # Perform LOOCV prediction with the selected features and model\n","                    for train_index, test_index in loo.split(X_reduced):\n","                        X_train, X_test = X_reduced.iloc[train_index][selected_features], X_reduced.iloc[test_index][selected_features]\n","                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                        # Use the appropriate model for prediction\n","                        if ml_model_name == 'RandomForest':\n","                            model = RandomForestClassifier(n_estimators=300, random_state=42)\n","                        else:\n","                            model = ElasticNetCV(cv=loo, max_iter=10000, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7)\n","\n","                        # Fit the model, make predictions and compute F1 score for the current fold\n","                        model.fit(X_train, y_train)\n","                        test_prediction = (model.predict_proba(X_test)[:, 1] >= threshold).astype(int) if ml_model_name == 'RandomForest' else (model.predict(X_test) >= threshold).astype(int)\n","                        f1_score_current = f1_score(y_test, test_prediction, average='macro')\n","                        fold_f1_scores.append(f1_score_current)\n","\n","                    # Calculate the average F1 score for the current threshold\n","                    f1_score_avg = np.mean(fold_f1_scores)\n","\n","                    # Record the best performance and threshold for the features\n","                    if f1_score_avg > best_performance_for_features:\n","                        best_performance_for_features = f1_score_avg\n","                        best_threshold_for_features = threshold\n","\n","                # Update the best overall performance if it improves\n","                if best_performance_for_features > best_overall_performance:\n","                    best_overall_performance = best_performance_for_features\n","                    best_overall_setup = {\n","                        'features': selected_features,\n","                        'threshold': best_threshold_for_features,\n","                        'performance': best_performance_for_features,\n","                        'feature_selection_method': feature_selector_name,\n","                        'ml_model': ml_model_name\n","                    }\n","\n","            return {\n","                'feature_selection_method': best_overall_setup['feature_selection_method'],\n","                'ml_model': best_overall_setup['ml_model'],\n","                'best_features': best_overall_setup['features'],\n","                'features_length': len(best_overall_setup['features']),\n","                'best_threshold': best_overall_setup['threshold'],\n","                'best_performance': best_overall_setup['performance']\n","            }\n","\n","        # Process combinations of feature selection and prediction models\n","        combinations = [('LASSO', 'ElasticNet'), ('ElasticNet', 'ElasticNet')]\n","        results_per_combination = [process_combination(fs, ml) for fs, ml in combinations]\n","        best_combination = max(results_per_combination, key=lambda x: x['best_performance'] if x is not None else 0)\n","\n","        return time_point, best_combination\n","\n","    # Process relative time points within the specified range\n","    time_points = np.sort(data['Relative_timepoint'].unique())\n","    results_parallel = Parallel(n_jobs=-1)(delayed(process_time_point)(tp) for tp in time_points)\n","    results = {tp: result for tp, result in results_parallel if result is not None}\n","\n","    return results\n","\n","# Evaluate the model\n","results_relative = evaluate_models(filtered_data)\n","\n","# Print results\n","print(\"Results for Celiac Disease Prediction by Relative Time Point:\")\n","for time_point, res in results_relative.items():\n","    if res is not None:\n","        print(f\"Relative Time Point: {time_point}\")\n","        print(f\"  Feature Selection Method: {res['feature_selection_method']}\")\n","        print(f\"  Machine Learning Model: {res['ml_model']}\")\n","        print(f\"  Best F1 Score: {res['best_performance']}\")\n","        print(f\"  Best Threshold: {res['best_threshold']}\")\n","        print(f\"  Features Used: {res['best_features']}\")\n","        print(f\"  Features Length: {res['features_length']}\")\n","        print(\"-\" * 40)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"elapsed":835,"status":"ok","timestamp":1716337861382,"user":{"displayName":"ZHIQI ZHANG","userId":"11425140851942185431"},"user_tz":240},"id":"Lm_onMZwtMO6","outputId":"65d0c69a-ce52-4932-b2f8-ad6e5058330b"},"outputs":[],"source":["# Create a line chart to show the F1 score for each time point\n","# Sorting the time points\n","sorted_species_time_points = [12, 15]\n","sorted_species_f1_scores = [91, 79]\n","\n","# Create the line chart\n","plt.figure(figsize=(8, 4))\n","\n","# Plot the line with points and set line color, marker size, and style\n","plt.plot(sorted_species_time_points, sorted_species_f1_scores, marker='o', linestyle='-', color='blue', markersize=10)\n","\n","# Add labels to each point\n","for i, score in enumerate(sorted_species_f1_scores):\n","    plt.text(sorted_species_time_points[i], score + 1, f\"{score}\", ha='center', va='bottom', fontsize=14)\n","\n","# Add titles and labels\n","plt.title('F1 Score by Age for CeD Prediction for All Subjects using Species Abundance Data', fontsize=14, color='blue', pad=20)\n","plt.xlabel('Age (Months)', fontsize=14)\n","plt.ylabel('Average F1 Score', fontsize=14)\n","\n","# Adjust the axis limits\n","plt.xticks(sorted_species_time_points, fontsize=12)\n","plt.ylim(0, 100)\n","plt.yticks(fontsize=12)\n","\n","# Show the plot with tight layout\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a line chart to show the F1 score for each time point\n","# Sorting the time points\n","sorted_species_time_points = [12, 15]\n","sorted_species_f1_scores = [100, 82]\n","\n","# Create the line chart\n","plt.figure(figsize=(8, 4))\n","\n","# Plot the line with points and set line color, marker size, and style\n","plt.plot(sorted_species_time_points, sorted_species_f1_scores, marker='o', linestyle='-', color='blue', markersize=10)\n","\n","# Add labels to each point\n","for i, score in enumerate(sorted_species_f1_scores):\n","    plt.text(sorted_species_time_points[i], score + 1, f\"{score}\", ha='center', va='bottom', fontsize=14)\n","\n","# Add titles and labels\n","plt.title('F1 Score by Age for Early vs. Late Onset Prediction using Species Abundance Data', fontsize=14, color='blue', pad=20)\n","plt.xlabel('Age (Months)', fontsize=14)\n","plt.ylabel('Average F1 Score', fontsize=14)\n","\n","# Adjust the axis limits\n","plt.xticks(sorted_species_time_points, fontsize=12)\n","plt.ylim(0, 100)\n","plt.yticks(fontsize=12)\n","\n","# Show the plot with tight layout\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a line chart to show the F1 score for each time point\n","# Sorting the time points\n","sorted_species_time_points = [-18, -15, -12, -9, -6, -3, 0]\n","sorted_species_f1_scores = [100, 100, 92, 91, 91, 91, 86]\n","\n","# Create the line chart\n","plt.figure(figsize=(8, 4))\n","\n","# Plot the line with points and set line color, marker size, and style\n","plt.plot(sorted_species_time_points, sorted_species_f1_scores, marker='o', linestyle='-', color='blue', markersize=10)\n","\n","# Add labels to each point\n","for i, score in enumerate(sorted_species_f1_scores):\n","    plt.text(sorted_species_time_points[i], score + 1, f\"{score}\", ha='center', va='bottom', fontsize=14)\n","\n","# Add titles and labels\n","plt.title('F1 Score by Relative Time Point for CeD Prediction using Species Abundance Data', fontsize=14, color='blue', pad=20)\n","plt.xlabel('Age (Months)', fontsize=14)\n","plt.ylabel('Average F1 Score', fontsize=14)\n","\n","# Adjust the axis limits\n","plt.xticks(sorted_species_time_points, fontsize=12)\n","plt.ylim(0, 100)\n","plt.yticks(fontsize=12)\n","\n","# Show the plot with tight layout\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
