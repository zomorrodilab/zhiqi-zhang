{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"YH20TaYMfOPx"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","    os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5mHH-frLYN_I"},"outputs":[],"source":["# Load packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import re\n","from collections import Counter, defaultdict\n","from joblib import Parallel, delayed\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectKBest, f_classif, RFE\n","from sklearn.linear_model import LassoCV, LogisticRegression, LogisticRegressionCV, ElasticNetCV, ElasticNet\n","from sklearn.metrics import f1_score, roc_auc_score\n","from sklearn.model_selection import LeaveOneOut, KFold, GridSearchCV, StratifiedKFold\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","import statistics\n","import xgboost as xgb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":5126,"status":"ok","timestamp":1716746521218,"user":{"displayName":"ZHIQI ZHANG","userId":"11425140851942185431"},"user_tz":240},"id":"zO7a90AvYqwL","outputId":"03b794b3-887f-4421-eda9-22048738a493"},"outputs":[],"source":["# Load dataset\n","file_path = 'pathway_abundance_merged_2024-08-21.xlsx'\n","new_data = pd.read_excel(file_path)\n","new_data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Exclude subjects with a CD onset of 12 months\n","excluded_subjects = [23, 31]\n","new_data = new_data[~new_data['Subject_number'].isin(excluded_subjects)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to apply abundance and prevalence thresholds\n","def filter_pathways(data, abundance_threshold = 0.001, prevalence_threshold = 0.1):\n","    initial_pathways_count = data.shape[1] - 8\n","    sample_count = data.shape[0]\n","    \n","    # Calculate prevalence threshold\n","    min_prevalent_samples = int(prevalence_threshold * sample_count)\n","\n","    # Filter pathways based on the thresholds\n","    pathways_columns = data.columns.difference(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'])\n","    pathways_data = data[pathways_columns]\n","    \n","    # Calculate the abundance and prevalence for each pathway\n","    pathways_above_threshold = (pathways_data >= abundance_threshold).sum(axis=0) >= min_prevalent_samples\n","    filtered_pathways = pathways_columns[pathways_above_threshold]\n","    \n","    # Filter the data to keep only the selected pathways\n","    filtered_data = data[['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'] + filtered_pathways.tolist()]\n","\n","    final_pathways_count = len(filtered_pathways)\n","    print(f\"Initial number of pathways: {initial_pathways_count}\")\n","    print(f\"Number of pathways after filtering: {final_pathways_count}\")\n","    \n","    return filtered_data\n","\n","# Filter the data\n","new_data_filtered = filter_pathways(new_data)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Divide subjects into early onset (â‰¤ 30 months) and late onset (> 30 months)\n","early_onset_subjects = [9, 20, 27, 29, 30, 35, 10, 21, 22, 36, 5, 13, 18, 25, 34]\n","\n","late_onset_subjects = [11, 24, 3, 12, 15, 17, 28, 32, 16, 1, 4, 6, 8, 14, 19, 2, 7]\n","\n","# Filter data for each group\n","early_onset_data = new_data_filtered[(new_data_filtered['Subject_number'].isin(early_onset_subjects)) & (new_data_filtered['timepoint_numeric'] < 18)]\n","late_onset_data = new_data_filtered[(new_data_filtered['Subject_number'].isin(late_onset_subjects)) & (new_data_filtered['timepoint_numeric'] < 36)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6178050,"status":"ok","timestamp":1716315664250,"user":{"displayName":"ZHIQI ZHANG","userId":"11425140851942185431"},"user_tz":240},"id":"F5_nys1ArUKp","outputId":"3952a28d-4c32-4b32-d3b7-b22f3ea861f1"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","# Define a function to evaluate combinations of feature selectors and models\n","def evaluate_models(data, max_timepoint):\n","    results = {}\n","\n","    # Define a function to process each time point independently\n","    def process_time_point(time_point):\n","        print(f\"Processing time point: {time_point}\")\n","\n","        # Filter data for the current time point and extract selected features\n","        current_data = data[data['timepoint_numeric'] == time_point]\n","        X = current_data.drop(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'], axis=1) # Feature matrix\n","        y = current_data['Diagnosis'] # Labels\n","\n","        # Ensure there are enough samples to perform LOOCV\n","        if len(y) < 2:\n","            print(f\"Skipping time point {time_point} due to insufficient samples.\")\n","            return time_point, None\n","\n","        # Initial feature reduction to 100 features\n","        selector = SelectKBest(score_func=f_classif, k=100)\n","        X_reduced = selector.fit_transform(X, y)\n","        selected_feature_names = X.columns[selector.get_support(indices=True)]\n","        X_reduced = pd.DataFrame(X_reduced, columns=selected_feature_names)\n","\n","        # Initialize Leave-One-Out cross-validation\n","        loo = LeaveOneOut()\n","        best_overall_score = 0\n","        best_overall_setup = {}\n","\n","        # Define a function to process each feature selector and model combination\n","        def process_combination(feature_selector_name, ml_model_name):\n","            all_selected_features = []\n","            all_importances = []\n","\n","            # Loop through the training/test splits generated by LOOCV\n","            for train_index, test_index in loo.split(X_reduced):\n","                X_train, X_test = X_reduced.iloc[train_index], X_reduced.iloc[test_index]\n","                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                # Perform feature selection based on the specified selector\n","                if feature_selector_name == 'LASSO':\n","                    feature_selector = LassoCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30)).fit(X_train, y_train)\n","                else:\n","                    feature_selector = ElasticNetCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7).fit(X_train, y_train)\n","\n","                # Select features with non-zero coefficients\n","                selected_features = X_train.columns[feature_selector.coef_ != 0]\n","                selected_features = selected_features[:min(len(selected_features), int(0.8 * len(y_train)))]\n","\n","                # If no features are selected, skip this iteration\n","                if len(selected_features) == 0:\n","                    continue\n","\n","                # Select the relevant features from the training set\n","                X_train_selected = X_train[selected_features]\n","\n","                # Perform logistic regression for ranking the selected features based on importance\n","                logistic = LogisticRegression(max_iter=10000, random_state=42, solver='liblinear').fit(X_train_selected, y_train)\n","                importances = abs(logistic.coef_[0])\n","                ranked_features = sorted(zip(selected_features, importances), key=lambda x: x[1], reverse=True)\n","\n","                # Store selected features and their importance\n","                all_selected_features.extend([f[0] for f in ranked_features])\n","                all_importances.extend([f[1] for f in ranked_features])\n","\n","            # If no features were selected across all folds, return None\n","            if not all_selected_features:\n","                return None\n","\n","            # Aggregate selected features across all folds\n","            unique_features = list(set(all_selected_features))\n","            frequency = Counter(all_selected_features)\n","            avg_importance = {feature: np.mean([imp for feat, imp in zip(all_selected_features, all_importances) if feat == feature])\n","                              for feature in unique_features}\n","\n","            # Calculate a composite score for each feature based on frequency and importance\n","            composite_scores = {feature: 0.5 * (frequency[feature] / loo.get_n_splits(X_reduced)) + 0.5 * (avg_importance[feature] / sum(avg_importance.values()))\n","                                for feature in unique_features}\n","            sorted_features = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","            # Evaluate different feature subsets with varying thresholds\n","            best_overall_performance = 0\n","            best_overall_setup = {}\n","            best_percentage = 0\n","            thresholds = np.linspace(0.05, 0.95, 19)\n","\n","            for i in range(1, min(len(sorted_features), int(0.8 * len(y))) + 1):\n","                selected_features = [feature[0] for feature in sorted_features[:i]]\n","                best_performance_for_features = 0\n","                best_threshold_for_features = None\n","\n","                for threshold in thresholds:\n","                    fold_f1_scores = []\n","\n","                    # Perform LOOCV prediction with the selected features and model\n","                    for train_index, test_index in loo.split(X_reduced):\n","                        X_train, X_test = X_reduced.iloc[train_index][selected_features], X_reduced.iloc[test_index][selected_features]\n","                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                        # Use the appropriate model for prediction\n","                        if ml_model_name == 'RandomForest':\n","                            model = RandomForestClassifier(n_estimators=300, random_state=42)\n","                        else:\n","                            model = ElasticNetCV(cv=5, max_iter=10000, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7)\n","\n","                        # Fit the model, make predictions and compute F1 score for the current fold\n","                        model.fit(X_train, y_train)\n","                        test_prediction = (model.predict_proba(X_test)[:, 1] >= threshold).astype(int) if ml_model_name == 'RandomForest' else (model.predict(X_test) >= threshold).astype(int)\n","                        f1_score_current = f1_score(y_test, test_prediction, average='macro')\n","                        fold_f1_scores.append(f1_score_current)\n","\n","                    # Calculate the average F1 score for the current threshold\n","                    f1_score_avg = np.mean(fold_f1_scores)\n","\n","                    # Record the best performance and threshold for the features\n","                    if f1_score_avg > best_performance_for_features:\n","                        best_performance_for_features = f1_score_avg\n","                        best_threshold_for_features = threshold\n","\n","                # Update the best overall performance if it improves\n","                if best_performance_for_features > best_overall_performance:\n","                    best_overall_performance = best_performance_for_features\n","                    best_overall_setup = {\n","                        'features': selected_features,\n","                        'threshold': best_threshold_for_features,\n","                        'performance': best_performance_for_features,\n","                        'feature_selection_method': feature_selector_name,\n","                        'ml_model': ml_model_name\n","                    }\n","\n","            return {\n","                'feature_selection_method': best_overall_setup['feature_selection_method'],\n","                'ml_model': best_overall_setup['ml_model'],\n","                'best_features': best_overall_setup['features'],\n","                'features_length': len(best_overall_setup['features']),\n","                'best_threshold': best_overall_setup['threshold'],\n","                'best_performance': best_overall_setup['performance']\n","            }\n","\n","        # Process combinations of feature selection and prediction models\n","        combinations = [('LASSO', 'ElasticNet'), ('ElasticNet', 'ElasticNet'), \n","                        ('LASSO', 'RandomForest'), ('ElasticNet', 'RandomForest')]\n","        results_per_combination = [process_combination(fs, ml) for fs, ml in combinations]\n","        best_combination = max(results_per_combination, key=lambda x: x['best_performance'] if x is not None else 0)\n","\n","        return time_point, best_combination\n","\n","    # Process time points within the specified max_timepoint\n","    time_points = np.sort(data[data['timepoint_numeric'] < max_timepoint]['timepoint_numeric'].unique())\n","    results_parallel = Parallel(n_jobs=-1)(delayed(process_time_point)(tp) for tp in time_points)\n","    results = {tp: result for tp, result in results_parallel if result is not None}\n","\n","    return results\n","\n","# Set the max timepoint for early onset groups\n","early_onset_max_timepoint = 18\n","\n","# Evaluate models for early onset group\n","print(\"Evaluating Early Onset Group\")\n","results_early_onset = evaluate_models(early_onset_data, early_onset_max_timepoint)\n","\n","# Print results for early onset group\n","print(\"Results for Early Onset Group:\")\n","for time_point, res in results_early_onset.items():\n","    if res is not None:\n","        print(f\"Time Point: {time_point}\")\n","        print(f\"  Feature Selection Method: {res['feature_selection_method']}\")\n","        print(f\"  Machine Learning Model: {res['ml_model']}\")\n","        print(f\"  Best F1 Score: {res['best_performance']}\")\n","        print(f\"  Best Threshold: {res['best_threshold']}\")\n","        print(f\"  Features Used: {res['best_features']}\")\n","        print(f\"  Features Length: {res['features_length']}\")\n","        print(\"-\" * 40)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","# Define a function to evaluate models\n","def evaluate_models(data, max_timepoint):\n","    results = {}\n","\n","    def process_time_point(time_point):\n","        print(f\"Processing time point: {time_point}\")\n","\n","        # Filter data for the current time point\n","        current_data = data[data['timepoint_numeric'] == time_point]\n","        X = current_data.drop(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Country', 'Diagnosis', 'CD_onset'], axis=1)\n","        y = current_data['Diagnosis']\n","\n","        # Check if there are enough samples for cross-validation\n","        if len(y) < 2:\n","            print(f\"Skipping time point {time_point} due to insufficient samples.\")\n","            return time_point, None\n","\n","        # Initial feature reduction to 100 features (move outside the CV loop)\n","        selector = SelectKBest(score_func=f_classif, k=100)\n","        X_reduced = selector.fit_transform(X, y)\n","        selected_feature_names = X.columns[selector.get_support(indices=True)]\n","        X_reduced = pd.DataFrame(X_reduced, columns=selected_feature_names)\n","\n","        # Initialize leave-one-out cross-validation\n","        loo = LeaveOneOut()\n","\n","        # Define a function to process each feature selector and model combination\n","        def process_combination(feature_selector_name, ml_model_name):\n","            all_selected_features = []\n","            all_importances = []\n","\n","            # Perform feature selection on the entire dataset (outside the CV loop)\n","            if feature_selector_name == 'LASSO':\n","                feature_selector = LassoCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30)).fit(X_reduced, y)\n","            else:\n","                feature_selector = ElasticNetCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7).fit(X_reduced, y)\n","\n","            selected_features = X_reduced.columns[feature_selector.coef_ != 0]\n","            selected_features = selected_features[:min(len(selected_features), int(0.8 * len(y)))]\n","\n","            if len(selected_features) == 0:\n","                return None\n","\n","            X_selected = X_reduced[selected_features]\n","\n","            # Perform logistic regression for ranking features (outside the CV loop)\n","            logistic = LogisticRegression(max_iter=10000, random_state=42, solver='liblinear').fit(X_selected, y)\n","            importances = abs(logistic.coef_[0])\n","            ranked_features = sorted(zip(selected_features, importances), key=lambda x: x[1], reverse=True)\n","\n","            # Now proceed to test different numbers of features and thresholds\n","            best_overall_performance = 0\n","            best_overall_setup = {}\n","            best_percentage = 0\n","            thresholds = np.linspace(0.05, 0.95, 19)\n","\n","            for i in range(1, min(len(ranked_features), int(0.8 * len(y))) + 1):\n","                selected_features_subset = [feature[0] for feature in ranked_features[:i]]\n","                best_performance_for_features = 0\n","                best_threshold_for_features = None\n","\n","                for threshold in thresholds:\n","                    fold_f1_scores = []\n","\n","                    # leave-one-out cross-validation loop (using consistent features)\n","                    for train_index, test_index in loo.split(X_reduced):\n","                        X_train = X_reduced.iloc[train_index][selected_features_subset]\n","                        X_test = X_reduced.iloc[test_index][selected_features_subset]\n","                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                        # Perform model prediction\n","                        if ml_model_name == 'RandomForest':\n","                            model = RandomForestClassifier(n_estimators=300, random_state=42)\n","                            model.fit(X_train, y_train)\n","                            test_prediction = (model.predict_proba(X_test)[:, 1] >= threshold).astype(int)\n","                        else:\n","                            model = ElasticNetCV(cv=5, max_iter=10000, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7)\n","                            model.fit(X_train, y_train)\n","                            test_prediction = (model.predict(X_test) >= threshold).astype(int)\n","\n","                        f1_score_current = f1_score(y_test, test_prediction, average='macro')\n","                        fold_f1_scores.append(f1_score_current)\n","\n","                    f1_score_avg = np.mean(fold_f1_scores)\n","\n","                    if f1_score_avg > best_performance_for_features:\n","                        best_performance_for_features = f1_score_avg\n","                        best_threshold_for_features = threshold\n","\n","                if best_performance_for_features > best_overall_performance:\n","                    best_overall_performance = best_performance_for_features\n","                    best_overall_setup = {\n","                        'features': selected_features_subset,\n","                        'threshold': best_threshold_for_features,\n","                        'performance': best_overall_performance,\n","                        'feature_selection_method': feature_selector_name,\n","                        'ml_model': ml_model_name\n","                    }\n","                    best_percentage = i / len(ranked_features)\n","\n","            return {\n","                'feature_selection_method': best_overall_setup['feature_selection_method'],\n","                'ml_model': best_overall_setup['ml_model'],\n","                'best_features': best_overall_setup['features'],\n","                'features_length': len(best_overall_setup['features']),\n","                'best_threshold': best_overall_setup['threshold'],\n","                'best_performance': best_overall_setup['performance'],\n","                'best_percentage': best_percentage\n","            }\n","\n","        # Process combinations of LASSO and ElasticNet for feature selection and RandomForest and ElasticNet for prediction\n","        combinations = [('LASSO', 'ElasticNet')]\n","        results_per_combination = [process_combination(fs, ml) for fs, ml in combinations]\n","        best_combination = max(results_per_combination, key=lambda x: x['best_performance'] if x is not None else 0)\n","\n","        return time_point, best_combination\n","\n","    # Only process time points within the specified max_timepoint\n","    time_points = np.sort(data[data['timepoint_numeric'] < max_timepoint]['timepoint_numeric'].unique())\n","    results_parallel = Parallel(n_jobs=-1)(delayed(process_time_point)(tp) for tp in time_points)\n","    results = {tp: result for tp, result in results_parallel if result is not None}\n","\n","    return results\n","\n","# Set the max timepoint for late onset groups\n","late_onset_max_timepoint = 36  # Only consider timepoints before 36 months for late onset\n","\n","# Evaluate models for late onset group\n","print(\"Evaluating Late Onset Group\")\n","results_late_onset = evaluate_models(late_onset_data, late_onset_max_timepoint)\n","\n","# Print results for late onset group\n","print(\"Results for Late Onset Group:\")\n","for time_point, res in results_late_onset.items():\n","    if res is not None:\n","        print(f\"Time Point: {time_point}\")\n","        print(f\"  Feature Selection Method: {res['feature_selection_method']}\")\n","        print(f\"  Machine Learning Model: {res['ml_model']}\")\n","        print(f\"  Best F1 Score: {res['best_performance']}\")\n","        print(f\"  Best Threshold: {res['best_threshold']}\")\n","        print(f\"  Features Used: {res['best_features']}\")\n","        print(f\"  Features Length: {res['features_length']}\")\n","        print(f\"  Best Percentage: {res['best_percentage']:.2%}\")\n","        print(\"-\" * 40)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","# Define a function to evaluate combinations of feature selectors and models\n","def evaluate_models(data, max_timepoint):\n","    results = {}\n","\n","    # Define a function to process each time point independently\n","    def process_time_point(time_point):\n","        print(f\"Processing time point: {time_point}\")\n","\n","        # Filter data for the current time point and extract selected features\n","        current_data = data[data['timepoint_numeric'] == time_point]\n","        X = current_data.drop(['SampleID', 'Subject', 'Subject_number', 'timepoint_numeric', 'Diagnosis', 'CD_onset', 'Relative_timepoint', 'Country'], axis=1) # Feature matrix\n","        y = current_data['Diagnosis'] # Labels\n","\n","        # Ensure there are enough samples to perform LOOCV\n","        if len(y) < 2:\n","            print(f\"Skipping time point {time_point} due to insufficient samples.\")\n","            return time_point, None\n","\n","        # Initial feature reduction to 100 features\n","        selector = SelectKBest(score_func=f_classif, k=100)\n","        X_reduced = selector.fit_transform(X, y)\n","        selected_feature_names = X.columns[selector.get_support(indices=True)]\n","        X_reduced = pd.DataFrame(X_reduced, columns=selected_feature_names)\n","\n","        # Initialize Leave-One-Out cross-validation\n","        loo = LeaveOneOut()\n","        best_overall_score = 0\n","        best_overall_setup = {}\n","\n","        # Define a function to process each feature selector and model combination\n","        def process_combination(feature_selector_name, ml_model_name):\n","            all_selected_features = []\n","            all_importances = []\n","\n","            # Loop through the training/test splits generated by LOOCV\n","            for train_index, test_index in loo.split(X_reduced):\n","                X_train, X_test = X_reduced.iloc[train_index], X_reduced.iloc[test_index]\n","                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                # Perform feature selection based on the specified selector\n","                if feature_selector_name == 'LASSO':\n","                    feature_selector = LassoCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30)).fit(X_train, y_train)\n","                else:\n","                    feature_selector = ElasticNetCV(cv=5, max_iter=20000, tol=1e-4, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7).fit(X_train, y_train)\n","\n","                # Select features with non-zero coefficients\n","                selected_features = X_train.columns[feature_selector.coef_ != 0]\n","                selected_features = selected_features[:min(len(selected_features), int(0.8 * len(y_train)))]\n","\n","                # If no features are selected, skip this iteration\n","                if len(selected_features) == 0:\n","                    continue\n","\n","                # Select the relevant features from the training set\n","                X_train_selected = X_train[selected_features]\n","\n","                # Perform logistic regression for ranking the selected features based on importance\n","                logistic = LogisticRegression(max_iter=10000, random_state=42, solver='liblinear').fit(X_train_selected, y_train)\n","                importances = abs(logistic.coef_[0])\n","                ranked_features = sorted(zip(selected_features, importances), key=lambda x: x[1], reverse=True)\n","\n","                # Store selected features and their importance\n","                all_selected_features.extend([f[0] for f in ranked_features])\n","                all_importances.extend([f[1] for f in ranked_features])\n","\n","            # If no features were selected across all folds, return None\n","            if not all_selected_features:\n","                return None\n","\n","            # Aggregate selected features across all folds\n","            unique_features = list(set(all_selected_features))\n","            frequency = Counter(all_selected_features)\n","            avg_importance = {feature: np.mean([imp for feat, imp in zip(all_selected_features, all_importances) if feat == feature])\n","                              for feature in unique_features}\n","\n","            # Calculate a composite score for each feature based on frequency and importance\n","            composite_scores = {feature: 0.5 * (frequency[feature] / loo.get_n_splits(X_reduced)) + 0.5 * (avg_importance[feature] / sum(avg_importance.values()))\n","                                for feature in unique_features}\n","            sorted_features = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","            # Evaluate different feature subsets with varying thresholds\n","            best_overall_performance = 0\n","            best_overall_setup = {}\n","            best_percentage = 0\n","            thresholds = np.linspace(0.05, 0.95, 19)\n","\n","            for i in range(1, min(len(sorted_features), int(0.8 * len(y))) + 1):\n","                selected_features = [feature[0] for feature in sorted_features[:i]]\n","                best_performance_for_features = 0\n","                best_threshold_for_features = None\n","\n","                for threshold in thresholds:\n","                    fold_f1_scores = []\n","\n","                    # Perform LOOCV prediction with the selected features and model\n","                    for train_index, test_index in loo.split(X_reduced):\n","                        X_train, X_test = X_reduced.iloc[train_index][selected_features], X_reduced.iloc[test_index][selected_features]\n","                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","                        # Use the appropriate model for prediction\n","                        if ml_model_name == 'RandomForest':\n","                            model = RandomForestClassifier(n_estimators=300, random_state=42)\n","                        else:\n","                            model = ElasticNetCV(cv=5, max_iter=10000, alphas=np.logspace(-6, -2, 30), l1_ratio=0.7)\n","\n","                        # Fit the model, make predictions and compute F1 score for the current fold\n","                        model.fit(X_train, y_train)\n","                        test_prediction = (model.predict_proba(X_test)[:, 1] >= threshold).astype(int) if ml_model_name == 'RandomForest' else (model.predict(X_test) >= threshold).astype(int)\n","                        f1_score_current = f1_score(y_test, test_prediction, average='macro')\n","                        fold_f1_scores.append(f1_score_current)\n","\n","                    # Calculate the average F1 score for the current threshold\n","                    f1_score_avg = np.mean(fold_f1_scores)\n","\n","                    # Record the best performance and threshold for the features\n","                    if f1_score_avg > best_performance_for_features:\n","                        best_performance_for_features = f1_score_avg\n","                        best_threshold_for_features = threshold\n","\n","                # Update the best overall performance if it improves\n","                if best_performance_for_features > best_overall_performance:\n","                    best_overall_performance = best_performance_for_features\n","                    best_overall_setup = {\n","                        'features': selected_features,\n","                        'threshold': best_threshold_for_features,\n","                        'performance': best_performance_for_features,\n","                        'feature_selection_method': feature_selector_name,\n","                        'ml_model': ml_model_name\n","                    }\n","\n","            return {\n","                'feature_selection_method': best_overall_setup['feature_selection_method'],\n","                'ml_model': best_overall_setup['ml_model'],\n","                'best_features': best_overall_setup['features'],\n","                'features_length': len(best_overall_setup['features']),\n","                'best_threshold': best_overall_setup['threshold'],\n","                'best_performance': best_overall_setup['performance']\n","            }\n","\n","        # Process combinations of feature selection and prediction models\n","        combinations = [('ElasticNet', 'ElasticNet')]\n","        results_per_combination = [process_combination(fs, ml) for fs, ml in combinations]\n","        best_combination = max(results_per_combination, key=lambda x: x['best_performance'] if x is not None else 0)\n","\n","        return time_point, best_combination\n","\n","    # Process time points within the specified max_timepoint\n","    time_points = np.sort(data[data['timepoint_numeric'] < max_timepoint]['timepoint_numeric'].unique())\n","    results_parallel = Parallel(n_jobs=-1)(delayed(process_time_point)(tp) for tp in time_points)\n","    results = {tp: result for tp, result in results_parallel if result is not None}\n","\n","    return results\n","\n","# Set the max timepoint for late onset groups\n","late_onset_max_timepoint = 36\n","\n","# Evaluate models for late onset group\n","print(\"Evaluating Late Onset Group\")\n","results_late_onset = evaluate_models(late_onset_data, late_onset_max_timepoint)\n","\n","# Print results for late onset group\n","print(\"Results for Late Onset Group:\")\n","for time_point, res in results_late_onset.items():\n","    if res is not None:\n","        print(f\"Time Point: {time_point}\")\n","        print(f\"  Feature Selection Method: {res['feature_selection_method']}\")\n","        print(f\"  Machine Learning Model: {res['ml_model']}\")\n","        print(f\"  Best F1 Score: {res['best_performance']}\")\n","        print(f\"  Best Threshold: {res['best_threshold']}\")\n","        print(f\"  Features Used: {res['best_features']}\")\n","        print(f\"  Features Length: {res['features_length']}\")\n","        print(\"-\" * 40)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"elapsed":835,"status":"ok","timestamp":1716337861382,"user":{"displayName":"ZHIQI ZHANG","userId":"11425140851942185431"},"user_tz":240},"id":"Lm_onMZwtMO6","outputId":"65d0c69a-ce52-4932-b2f8-ad6e5058330b"},"outputs":[],"source":["# Create a line chart to show the F1 score for each time point\n","# Sorting the time points\n","sorted_pathways_time_points = [12, 15]\n","sorted_pathways_f1_scores = [90, 80]\n","\n","# Create the line chart\n","plt.figure(figsize=(8, 4))\n","\n","# Plot the line with points and set line color, marker size, and style\n","plt.plot(sorted_pathways_time_points, sorted_pathways_f1_scores, marker='o', linestyle='-', color='blue', markersize=10)\n","\n","# Add labels to each point\n","for i, score in enumerate(sorted_pathways_f1_scores):\n","    plt.text(sorted_pathways_time_points[i], score + 1, f\"{score}\", ha='center', va='bottom', fontsize=14)\n","\n","# Add titles and labels\n","plt.title('F1 Score by Age for Early Onset Pathways Abundance Data', fontsize=14, color='blue', pad=20)\n","plt.xlabel('Age (Months)', fontsize=14)\n","plt.ylabel('Average F1 Score', fontsize=14)\n","\n","# Adjust the axis limits\n","plt.xticks(sorted_pathways_time_points, fontsize=12)\n","plt.ylim(0, 100)\n","plt.yticks(fontsize=12)\n","\n","# Show the plot with tight layout\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a line chart to show the F1 score for each time point\n","# Sorting the time points\n","sorted_pathways_time_points = [12, 15, 18, 21, 24, 27, 30, 33]\n","sorted_pathways_f1_scores = [82, 82, 79, 74, 88, 100, 82, 80]\n","\n","# Create the line chart\n","plt.figure(figsize=(8, 4))\n","\n","# Plot the line with points and set line color, marker size, and style\n","plt.plot(sorted_pathways_time_points, sorted_pathways_f1_scores, marker='o', linestyle='-', color='blue', markersize=10)\n","\n","# Add labels to each point\n","for i, score in enumerate(sorted_pathways_f1_scores):\n","    plt.text(sorted_pathways_time_points[i], score + 1, f\"{score}\", ha='center', va='bottom', fontsize=14)\n","\n","# Add titles and labels\n","plt.title('F1 Score by Age for Late Onset Pathways Abundance Data', fontsize=14, color='blue', pad=20)\n","plt.xlabel('Age (Months)', fontsize=14)\n","plt.ylabel('Average F1 Score', fontsize=14)\n","\n","# Adjust the axis limits\n","plt.xticks(sorted_pathways_time_points, fontsize=12)\n","plt.ylim(0, 100)\n","plt.yticks(fontsize=12)\n","\n","# Show the plot with tight layout\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
